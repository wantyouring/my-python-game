랜덤평균은 약 10정도

model1
frame skip 5단위(5
64 64 3 dense layer.
=> 학습 잘 안됨

model2
frame skip 4단위
256 128 3 dense layer
=> 2400epi까지 학습 잘 안됨.
avg qmax가 일정 이상 안올라감. 더 큰 layer써보기.

lr늘리고 update_target_rate줄여 다시 학습해보기.
랜덤으로 떨어지는 똥의 위치를 특정 n개 위치에서만 떨어지게끔 환경 바꾸기!

model3
1024 512 3 dense layer로 학습
=> 3000epi까지 학습해놓은거 이어서 학습시켜보기. 현재까지는 그냥 random보다 살짝 좋은정도...lr *1/10 더 줄여보기.

model4
frame skip 없이 학습시켜보자.
256 256 3 dense layer
=> model3보다 더 안됨.

model3이어서 학습중
=> 학습 잘 되지 않는다.
환경을 좀 더 단순화 시켜보기. 시작 y위치도 일정한 몇 개 위치중 랜덤하게 떨어지게끔. 

model5
시작 똥 y위치도 20개의 위치중 10개 위치 고르기. ddong_speed도 시작 y위치간격과 동일하게.
=> 학습 잘 안됨.

model6
시작 똥 x,y위치 모두 고정한 환경에서 학습되는지 테스트.
64 64 3 dense layer
dx 48로 크게 움직이게.(조금씩 여러 action이 모여 reward에 영향 미칠것같음), frame skip 없이.
=> 여기서 계속 파라미터, dx(24 -> 48) 등 바꿔가며 시도중.
lr 많이 줄여 해보기. 정밀한 fitting.

32 32 3 dense layer는 학습 안됨.

64 64 layer에서 55000epi쯤에서 학습완료됨. but play시 잘 안됨. 이어서 더 학습시켜보기. 학습완료조건 없이.

=> 학습 잘 안되었음.


model7
똥 갯수 조금 줄여보기.
=> 학습됨.

-----------------------------------------------
RL2_avoid_game.py와 cnn_doubleDQN.py에서 cnn적용하였음.
model1 : 
model = Sequential()
        model.add(Conv2D(16,(4,4),strides=(2,2),activation='relu',input_shape=self.state_size))
        model.add(Conv2D(32,(2,2),strides=(1,1),activation='relu'))
        model.add(Flatten())
        model.add(Dense(256,activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model
=> 오래걸려 일단 1500epi까지 학습 후 보류.
model2
model = Sequential()
        model.add(Conv2D(2,(4,4),strides=(2,2),activation='relu',input_shape=self.state_size))
        model.add(Conv2D(2,(2,2),strides=(1,1),activation='relu'))
        model.add(Flatten())
        model.add(Dense(256,activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model
필터 갯수 줄여봄.
3000
3000+3000
3000+3000+10000epi => 학습 잘 안됨.

model3
    def build_model(self):
        model = Sequential()
        model.add(Conv2D(4,(4,4),strides=(2,2),activation='relu',input_shape=self.state_size))
        model.add(Conv2D(8,(2,2),strides=(1,1),activation='relu'))
        model.add(Flatten())
        model.add(Dense(256,activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model

=> 잘 안됨.
lr bigger or smaller...(아직 학습 안해봄)

model4
똥 255, 사람 180, 빈칸 0으로 초기화 해보기. 

model5
필터 수 줄여서. => 잘 안됨. 필터 늘리는게 훨씬 잘 됨.

model6
필터 8,16,16
6500epi까지 학습. lr줄이고 이어서 학습중@.

최근 30epi평균이 10~20에서 더이상 올라가지 않는 문제.

model7 (저장x)
update_target_rate 늘려보기
똥 떨어졌을 시 reward 1 주는거 없앰.
+최고점 score 기록하는 model 계속 저장해주기. => 잘 동작 안함.

model8
max pooling 넣어보기.

model9
-----------RL2_avoid_game_multi_agent.py와 cnn_doubleDQN.py사용.------------------=> 여러 agent로 학습.
일정 epi_step마다 학습 agent바꿔 구간별로 학습하기. 여러 agent사용. epi_step 50마다 agent change.
avg_max_q 의미없음.
=> 일정 score 이상 올라가지 않음.

model10
epi_step 30마다 agent change.
=> 10점부터 score 다시 떨어짐. lr낮춰보기.
=> 15점부터 score 다시 떨어짐.  어디가 문제인가...

model11
똥 위치 바꿔서 학습.

----------------------RL2_avoid_game.py와 cnn_doubleDQN.py에서 cnn적용하였음.------
model11
memory(replay buffer)를 500000으로 키움. 이게 문제같다.
buffer에 게임 전체 시나리오가 아닌 앞의 일부분, 또는 학습이 잘 되어갈 시 게임 중간 부분까지의 내용만 buffer에 들어가 학습이 잘 되지 않는듯함. 일단 500000으로 키워 결과 확인해보고 내일 더 키워 테스트해보기. 기본 DQN replay buffer 크기가 100만이라고 함.

파이참 기본 메모리 800MB 사용.
5만 memory찼을때 전체 1500MB 사용. 메모리로 약 700MB 사용
50만 memory => 800+700*10 = 약8기가. 크롬 다 닫으면 아슬아슬.

https://github.com/reinforcement-learning-kr/how_to_study_rl/wiki/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B4%80%EB%A0%A8-%EB%85%B8%ED%95%98%EC%9A%B0
참고

=> 50만 메모리 딱 8기가 사용한다.
아직도 학습 잘 안됨.

model12
discount factor 키워서 학습해보자! 뒤에 먼 보상까지 보고 계산.
0.99 => 0.99999
e min 줄여 이어서 학습하자. 탐험이 필요 없는 환경이다.@@@@@@