랜덤평균은 약 10정도

model1
frame skip 5단위(5
64 64 3 dense layer.
=> 학습 잘 안됨

model2
frame skip 4단위
256 128 3 dense layer
=> 2400epi까지 학습 잘 안됨.
avg qmax가 일정 이상 안올라감. 더 큰 layer써보기.

lr늘리고 update_target_rate줄여 다시 학습해보기.
랜덤으로 떨어지는 똥의 위치를 특정 n개 위치에서만 떨어지게끔 환경 바꾸기!

model3
1024 512 3 dense layer로 학습
=> 3000epi까지 학습해놓은거 이어서 학습시켜보기. 현재까지는 그냥 random보다 살짝 좋은정도...lr *1/10 더 줄여보기.

model4
frame skip 없이 학습시켜보자.
256 256 3 dense layer
=> model3보다 더 안됨.

model3이어서 학습중
=> 학습 잘 되지 않는다.
환경을 좀 더 단순화 시켜보기. 시작 y위치도 일정한 몇 개 위치중 랜덤하게 떨어지게끔. 

model5
시작 똥 y위치도 20개의 위치중 10개 위치 고르기. ddong_speed도 시작 y위치간격과 동일하게.
=> 학습 잘 안됨.

model6
시작 똥 x,y위치 모두 고정한 환경에서 학습되는지 테스트.
64 64 3 dense layer
dx 48로 크게 움직이게.(조금씩 여러 action이 모여 reward에 영향 미칠것같음), frame skip 없이.
=> 여기서 계속 파라미터, dx(24 -> 48) 등 바꿔가며 시도중.
lr 많이 줄여 해보기. 정밀한 fitting.

32 32 3 dense layer는 학습 안됨.

64 64 layer에서 55000epi쯤에서 학습완료됨. but play시 잘 안됨. 이어서 더 학습시켜보기. 학습완료조건 없이.

=> 학습 잘 안되었음.


model7
똥 갯수 조금 줄여보기.
=> 학습됨.

-----------------------------------------------
RL2_avoid_game.py와 cnn_doubleDQN.py에서 cnn적용하였음.
model1 : 
model = Sequential()
        model.add(Conv2D(16,(4,4),strides=(2,2),activation='relu',input_shape=self.state_size))
        model.add(Conv2D(32,(2,2),strides=(1,1),activation='relu'))
        model.add(Flatten())
        model.add(Dense(256,activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model
=> 오래걸려 일단 1500epi까지 학습 후 보류.
model2
model = Sequential()
        model.add(Conv2D(2,(4,4),strides=(2,2),activation='relu',input_shape=self.state_size))
        model.add(Conv2D(2,(2,2),strides=(1,1),activation='relu'))
        model.add(Flatten())
        model.add(Dense(256,activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model
필터 갯수 줄여봄.
3000
3000+3000
3000+3000+10000epi => 학습 잘 안됨.

model3
    def build_model(self):
        model = Sequential()
        model.add(Conv2D(4,(4,4),strides=(2,2),activation='relu',input_shape=self.state_size))
        model.add(Conv2D(8,(2,2),strides=(1,1),activation='relu'))
        model.add(Flatten())
        model.add(Dense(256,activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model

=> 잘 안됨.
lr bigger or smaller...(아직 학습 안해봄)

model4
똥 255, 사람 180, 빈칸 0으로 초기화 해보기. 

model5
필터 수 줄여서. => 잘 안됨. 필터 늘리는게 훨씬 잘 됨.

model6
필터 8,16,16
6500epi까지 학습. lr줄이고 이어서 학습중@.

최근 30epi평균이 10~20에서 더이상 올라가지 않는 문제.

model7 (저장x)
update_target_rate 늘려보기
똥 떨어졌을 시 reward 1 주는거 없앰.
+최고점 score 기록하는 model 계속 저장해주기. => 잘 동작 안함.

model8
max pooling 넣어보기.

model9
-----------RL2_avoid_game_multi_agent.py와 cnn_doubleDQN.py사용.------------------=> 여러 agent로 학습.
일정 epi_step마다 학습 agent바꿔 구간별로 학습하기. 여러 agent사용. epi_step 50마다 agent change.
avg_max_q 의미없음.
=> 일정 score 이상 올라가지 않음.

model10
epi_step 30마다 agent change.
=> 10점부터 score 다시 떨어짐. lr낮춰보기.
=> 15점부터 score 다시 떨어짐.  어디가 문제인가...

model11
똥 위치 바꿔서 학습.

----------------------RL2_avoid_game.py와 cnn_doubleDQN.py에서 cnn적용하였음.------
model11
memory(replay buffer)를 500000으로 키움. 이게 문제같다.
buffer에 게임 전체 시나리오가 아닌 앞의 일부분, 또는 학습이 잘 되어갈 시 게임 중간 부분까지의 내용만 buffer에 들어가 학습이 잘 되지 않는듯함. 일단 500000으로 키워 결과 확인해보고 내일 더 키워 테스트해보기. 기본 DQN replay buffer 크기가 100만이라고 함.

파이참 기본 메모리 800MB 사용.
5만 memory찼을때 전체 1500MB 사용. 메모리로 약 700MB 사용
50만 memory => 800+700*10 = 약8기가. 크롬 다 닫으면 아슬아슬.

https://github.com/reinforcement-learning-kr/how_to_study_rl/wiki/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B4%80%EB%A0%A8-%EB%85%B8%ED%95%98%EC%9A%B0
참고

=> 50만 메모리 딱 8기가 사용한다.
아직도 학습 잘 안됨.

model12
discount factor 키워서 학습해보자! 뒤에 먼 보상까지 보고 계산.
0.99 => 0.99999
e min 줄여 이어서 학습하자. 탐험이 필요 없는 환경이다.
=> 학습 잘 안됨.

model13
추가할것: reward -100말고 -1로 바꾸기. initializer써보기(he). lr다시 줄여보기. 손실함수로 huber loss사용하기. 
=> 학습 잘 안됨.
@@@살짝 변형해 학습중.

----------100 line dx부분 잘못 코딩되어있었음!!!--------
이제 학습 잘 됨!!! => success_model에 저장. (model 13)
이제 화면 원래size로 키우고, 랜덤위치 똥 환경도 학습해보자.

model14
화면 원래size로 키움.
77,500,000 param num. => maxpooling처음에 넣어줌으로써 2,800,000로 줄임.
화면 size를 키우니 env를 state로 reshape하는데 시간이 너무 오래걸린다.
작은 화면리스트정보 넘겨주고 rendering할 때만 키워 보여주는식으로 해야할듯함. 이거로는 학습 불가능!
기존 크기의 가로*10, 세로*10이라 100배 커져 오래 걸릴만하다.

14-3
man, ddong, pad, speed 좌표들의 최소공배수40으로 나눠서(최대로 줄일 수 있는 픽셀단위)로 simple state구성해 학습.
=>학습완료

model15
랜덤한 똥drop. 
180,000epi 학습. render playing결과 어느정도 피하나 매우 초보레벨. 

model15-2
model dense layer 1024,1024로 키움.
=> aws에서 학습중@
현재 36000epi인데 초보레벨로 피하나 거의 금방 죽음. 많이 학습하면 잘 피할듯함. 그러나 model크기가 맞을지 잘 모르겠음. model 바꿔서, parameter바꿔서 테스트해보기.

model15-3
cnn model layer추가, 키워 학습.
layer에 padding='same'옵션 추가해주어야 오류 안생김.
2700epi model우선 save. 학습 더 잘 되는듯함. 그래프 한페이지에 그리게 코드수정 후 재학습.

model15-3-2
epsilon min 0.01로 줄이고 lr 0.0001로 늘림.
4300epi(2700 + 1600)epi까지 학습.
조금씩 평균점수 올라가는중. 똥이 운좋게 잘 떨어지면 점수가 꽤 나오는 경우도 있다!

model 15-3-3
aws에서 이어서 오래 학습시켜보자.
 => 이전보다는 꽤 학습되었고 점수도 나오나 avg q max가 올라가다 떨어짐.
=> aws에서 학습중@

model 15-4
model 더 키웠음.
=> 로컬에서 학습중.@