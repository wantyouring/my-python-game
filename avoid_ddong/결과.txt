랜덤평균은 약 10정도

model1
frame skip 5단위(5
64 64 3 dense layer.
=> 학습 잘 안됨

model2
frame skip 4단위
256 128 3 dense layer
=> 2400epi까지 학습 잘 안됨.
avg qmax가 일정 이상 안올라감. 더 큰 layer써보기.

lr늘리고 update_target_rate줄여 다시 학습해보기.
랜덤으로 떨어지는 똥의 위치를 특정 n개 위치에서만 떨어지게끔 환경 바꾸기!

model3
1024 512 3 dense layer로 학습
=> 3000epi까지 학습해놓은거 이어서 학습시켜보기. 현재까지는 그냥 random보다 살짝 좋은정도...lr *1/10 더 줄여보기.

model4
frame skip 없이 학습시켜보자.
256 256 3 dense layer
=> model3보다 더 안됨.

model3이어서 학습중
=> 학습 잘 되지 않는다.
환경을 좀 더 단순화 시켜보기. 시작 y위치도 일정한 몇 개 위치중 랜덤하게 떨어지게끔. 

model5
시작 똥 y위치도 20개의 위치중 10개 위치 고르기. ddong_speed도 시작 y위치간격과 동일하게.
=> 학습 잘 안됨.

model6
시작 똥 x,y위치 모두 고정한 환경에서 학습되는지 테스트.
64 64 3 dense layer
dx 48로 크게 움직이게.(조금씩 여러 action이 모여 reward에 영향 미칠것같음), frame skip 없이.
=> 여기서 계속 파라미터, dx(24 -> 48) 등 바꿔가며 시도중.
lr 많이 줄여 해보기. 정밀한 fitting.

32 32 3 dense layer는 학습 안됨.

64 64 layer에서 55000epi쯤에서 학습완료됨. but play시 잘 안됨. 이어서 더 학습시켜보기. 학습완료조건 없이.

=> 학습 잘 안되었음.


model7
똥 갯수 조금 줄여보기.
=> 학습됨.

-----------------------------------------------
RL2_avoid_game.py와 cnn_doubleDQN.py에서 cnn적용하였음.
model1 : 
model = Sequential()
        model.add(Conv2D(16,(4,4),strides=(2,2),activation='relu',input_shape=self.state_size))
        model.add(Conv2D(32,(2,2),strides=(1,1),activation='relu'))
        model.add(Flatten())
        model.add(Dense(256,activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model
=> 오래걸려 일단 1500epi까지 학습 후 보류.
model2
model = Sequential()
        model.add(Conv2D(2,(4,4),strides=(2,2),activation='relu',input_shape=self.state_size))
        model.add(Conv2D(2,(2,2),strides=(1,1),activation='relu'))
        model.add(Flatten())
        model.add(Dense(256,activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model
필터 갯수 줄여봄.
3000
3000+3000
3000+3000+10000epi => 학습 잘 안됨.

model3
    def build_model(self):
        model = Sequential()
        model.add(Conv2D(4,(4,4),strides=(2,2),activation='relu',input_shape=self.state_size))
        model.add(Conv2D(8,(2,2),strides=(1,1),activation='relu'))
        model.add(Flatten())
        model.add(Dense(256,activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model

=> 잘 안됨.
lr bigger or smaller...(아직 학습 안해봄)

model4
똥 255, 사람 180, 빈칸 0으로 초기화 해보기. 

model5
필터 수 줄여서. => 잘 안됨. 필터 늘리는게 훨씬 잘 됨.

model6
필터 8,16,16
6500epi까지 학습. lr줄이고 이어서 학습중@.

최근 30epi평균이 10~20에서 더이상 올라가지 않는 문제.

model7
update_target_rate 늘려보기

model8
똥 떨어졌을 시 reward 1 주는거 없앰.
+최고점 score 기록하는 model 계속 저장해주기.